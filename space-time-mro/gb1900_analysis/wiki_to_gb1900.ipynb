{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from timeit import default_timer as timer\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import Point\n",
    "from descartes import PolygonPatch\n",
    "from geopandas.tools import sjoin\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pyproj\n",
    "import haversine\n",
    "from scipy import spatial\n",
    "import getpass\n",
    "import os.path\n",
    "import fiona\n",
    "import json\n",
    "import glob\n",
    "import ast\n",
    "import csv\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the gazetteer DB server\n",
    "\n",
    "Make sure you change your credentials. This is to connect to the DB locally in MySQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Credentials from the credentials.json file\n",
    "credentials_config = dict()\n",
    "with open('./credentials.json') as f:\n",
    "    credentials_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credentials_config['lwmrelationaldb']['password'] = getpass.getpass(prompt='Enter your password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct connection string\n",
    "psql_conn_string =\\\n",
    "    \"host={0} user={1} dbname={2} password={3} sslmode={4} sslrootcert={5}\".format(\n",
    "                credentials_config['lwmrelationaldb']['host'], \n",
    "                credentials_config['lwmrelationaldb']['user'], \n",
    "                'gazetteer', \n",
    "                credentials_config['lwmrelationaldb']['password'], \n",
    "                credentials_config['lwmrelationaldb']['sslmode'], \n",
    "                credentials_config['lwmrelationaldb']['sslrootcert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established!\n"
     ]
    }
   ],
   "source": [
    "gazDB = psycopg2.connect(psql_conn_string) \n",
    "print(\"Connection established!\")\n",
    "\n",
    "cursorGaz = gazDB.cursor(cursor_factory=psycopg2.extras.DictCursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read GB1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khosseini/anaconda3/envs/py37torch/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "gb1900df = pd.DataFrame()\n",
    "with open(\"./gb1900_gazetteer_complete_july_2018.csv\", encoding='UTF-16') as f:\n",
    "    gb1900df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pin_id</th>\n",
       "      <th>final_text</th>\n",
       "      <th>nation</th>\n",
       "      <th>local_authority</th>\n",
       "      <th>parish</th>\n",
       "      <th>osgb_east</th>\n",
       "      <th>osgb_north</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>52b34d8b695fe90005004e1e</td>\n",
       "      <td>F. P.</td>\n",
       "      <td>Wales</td>\n",
       "      <td>Powys</td>\n",
       "      <td>Llansilin</td>\n",
       "      <td>320836.712742</td>\n",
       "      <td>327820.182715</td>\n",
       "      <td>52.842050</td>\n",
       "      <td>-3.176744</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5800a6b92c66dcab3d061796</td>\n",
       "      <td>Parly. &amp; Munl Boro. By.</td>\n",
       "      <td>England</td>\n",
       "      <td>City of London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>531794.825962</td>\n",
       "      <td>180705.741898</td>\n",
       "      <td>51.509918</td>\n",
       "      <td>-0.102246</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5800a6782c66dcab3d061786</td>\n",
       "      <td>S. Ps.</td>\n",
       "      <td>England</td>\n",
       "      <td>City of London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>531736.217116</td>\n",
       "      <td>180725.027730</td>\n",
       "      <td>51.510105</td>\n",
       "      <td>-0.103083</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>57f684f42c66dcab3d01c0dd</td>\n",
       "      <td>Southwark Bridge Stairs</td>\n",
       "      <td>England</td>\n",
       "      <td>City of London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>532199.584123</td>\n",
       "      <td>180696.934434</td>\n",
       "      <td>51.509744</td>\n",
       "      <td>-0.096420</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>57f685002c66dcab3d01c0e9</td>\n",
       "      <td>St. Paul's Pier</td>\n",
       "      <td>England</td>\n",
       "      <td>City of London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>531987.486097</td>\n",
       "      <td>180745.664556</td>\n",
       "      <td>51.510232</td>\n",
       "      <td>-0.099456</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pin_id               final_text   nation local_authority  \\\n",
       "0  52b34d8b695fe90005004e1e                    F. P.    Wales           Powys   \n",
       "1  5800a6b92c66dcab3d061796  Parly. & Munl Boro. By.  England  City of London   \n",
       "2  5800a6782c66dcab3d061786                   S. Ps.  England  City of London   \n",
       "3  57f684f42c66dcab3d01c0dd  Southwark Bridge Stairs  England  City of London   \n",
       "4  57f685002c66dcab3d01c0e9          St. Paul's Pier  England  City of London   \n",
       "\n",
       "      parish      osgb_east     osgb_north   latitude  longitude notes  \n",
       "0  Llansilin  320836.712742  327820.182715  52.842050  -3.176744   NaN  \n",
       "1        NaN  531794.825962  180705.741898  51.509918  -0.102246   NaN  \n",
       "2        NaN  531736.217116  180725.027730  51.510105  -0.103083   NaN  \n",
       "3        NaN  532199.584123  180696.934434  51.509744  -0.096420   NaN  \n",
       "4        NaN  531987.486097  180745.664556  51.510232  -0.099456   NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb1900df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an approximately British Wiki Gazetteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_british_locations(cursorGaz, timer):\n",
    "    print('Start locations SQL query: {} seconds'.format(timer() - start_time))\n",
    "\n",
    "    cursorGaz.execute(\"\"\"\n",
    "            SELECT location.*, inlinks.inlinks FROM location\n",
    "            JOIN inlinks ON inlinks.main_id=location.id\n",
    "            WHERE lat > 50.0\n",
    "            AND lat < 62.0\n",
    "            AND lon > -14.0\n",
    "            AND lon < 3.0\n",
    "        \"\"\")\n",
    "    results = cursorGaz.fetchall()\n",
    "    \n",
    "    main_id = []\n",
    "    wiki_title = []\n",
    "    wiki_lat = []\n",
    "    wiki_lon = []\n",
    "    page_len = []\n",
    "    type_loc = []\n",
    "    population = []\n",
    "    for r in results:\n",
    "        main_id.append(r['id'])\n",
    "        wiki_title.append(r['wiki_title'])\n",
    "        page_len.append(r['page_len'])\n",
    "        wiki_lat.append(r['lat'])\n",
    "        wiki_lon.append(r['lon'])\n",
    "        type_loc.append(r['type'])\n",
    "        population.append(r['population'])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {'main_id' : main_id,\n",
    "         'wiki_title': wiki_title,\n",
    "         'wiki_lat': wiki_lat,\n",
    "         'wiki_lon': wiki_lon,\n",
    "         'page_len': page_len,\n",
    "         'type_loc': type_loc,\n",
    "         'population': population\n",
    "        })\n",
    "    return df\n",
    "\n",
    "def gaz_to_geodataframe(df):\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, geometry=gpd.points_from_xy(df.wiki_lon, df.wiki_lat))\n",
    "\n",
    "    poly  = gpd.GeoDataFrame.from_file('gb1900_analysis/shapefiles/GBR_adm/GBR_adm0.shp')\n",
    "    point = gdf\n",
    "\n",
    "    pointInPolys = sjoin(point, poly, how='left')\n",
    "    grouped = pointInPolys.groupby('index_right')\n",
    "    \n",
    "    britdf = df.iloc[grouped.groups[0]]\n",
    "    britdf.reset_index(drop=True)\n",
    "    britdf.to_pickle(\"brit_wikigazetteer.pkl\")\n",
    "    \n",
    "    return britdf\n",
    "\n",
    "def find_british_altnames(cursorGaz, timer):\n",
    "    print('Start altnames SQL query: {} seconds'.format(timer() - start_time))\n",
    "\n",
    "    cursorGaz.execute(\"\"\"\n",
    "            SELECT altname.* FROM altname\n",
    "            JOIN location ON location.id=altname.main_id\n",
    "            WHERE lat > 50.0\n",
    "            AND lat < 62.0\n",
    "            AND lon > -14.0\n",
    "            AND lon < 3.0\n",
    "        \"\"\")\n",
    "    results = cursorGaz.fetchall()\n",
    "    \n",
    "    dAltnames = dict()\n",
    "    \n",
    "    for r in results:\n",
    "        if len(r['altname']) < 50:\n",
    "            dAltnames[r['id']] = (r['altname'], r['source'], r['main_id'])\n",
    "\n",
    "    return dAltnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locations dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start locations SQL query: 1.1958821369917132 seconds\n"
     ]
    },
    {
     "ename": "DriverError",
     "evalue": "gb1900_analysis/shapefiles/GBR_adm/GBR_adm0.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: gb1900_analysis/shapefiles/GBR_adm/GBR_adm0.shp: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-da9d11075adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlocdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_british_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursorGaz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbritdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgaz_to_geodataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#britdf.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbritdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./brit_wikigazetteer.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbritdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-87131a8cfb62>\u001b[0m in \u001b[0;36mgaz_to_geodataframe\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     43\u001b[0m         df, geometry=gpd.points_from_xy(df.wiki_lon, df.wiki_lat))\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mpoly\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeoDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gb1900_analysis/shapefiles/GBR_adm/GBR_adm0.shp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/src/geopandas/geopandas/geodataframe.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeoDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nybb.shp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \"\"\"\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/src/geopandas/geopandas/io/file.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, bbox, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37torch/lib/python3.6/site-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37torch/lib/python3.6/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[0;32m--> 253\u001b[0;31m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37torch/lib/python3.6/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: gb1900_analysis/shapefiles/GBR_adm/GBR_adm0.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "locdf = find_british_locations(cursorGaz, timer)\n",
    "britdf = gaz_to_geodataframe(locdf)\n",
    "#britdf.head()\n",
    "britdf = pd.read_pickle(\"./brit_wikigazetteer.pkl\")\n",
    "britdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Altnames dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAltnames = find_british_altnames(cursorGaz, timer)\n",
    "\n",
    "alt_id = []\n",
    "main_id = []\n",
    "altname = []\n",
    "source = []\n",
    "for r in dAltnames:\n",
    "    main_id.append(dAltnames[r][2])\n",
    "    alt_id.append(r)\n",
    "    altname.append(dAltnames[r][0])\n",
    "    source.append(dAltnames[r][1])\n",
    "\n",
    "altdf = pd.DataFrame(\n",
    "    {'alt_id' : alt_id,\n",
    "     'main_id': main_id,\n",
    "     'altname': altname,\n",
    "     'source': source\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altdf = altdf.groupby(\"main_id\")['altname'].apply(', '.join)\n",
    "altdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint locations and altnames dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "britdf = pd.merge(locdf, altdf, how='left', on='main_id')\n",
    "britdf = britdf[britdf.altname.notnull()]\n",
    "# britdf.head()\n",
    "\n",
    "# Example of multiple altnames:\n",
    "britdf[britdf['main_id'] == 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecef = pyproj.Proj(proj='geocent', ellps='WGS84', datum='WGS84')\n",
    "lla = pyproj.Proj(proj='latlong', ellps='WGS84', datum='WGS84')\n",
    "x, y, z = pyproj.transform(lla, ecef, \n",
    "                           gb1900df[\"longitude\"].to_numpy(), \n",
    "                           gb1900df[\"latitude\"].to_numpy(), \n",
    "                           np.zeros(len(gb1900df[\"latitude\"])), \n",
    "                           radians=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1900df[\"x\"] = x\n",
    "gb1900df[\"y\"] = y\n",
    "gb1900df[\"z\"] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecef = pyproj.Proj(proj='geocent', ellps='WGS84', datum='WGS84')\n",
    "lla = pyproj.Proj(proj='latlong', ellps='WGS84', datum='WGS84')\n",
    "x, y, z = pyproj.transform(lla, ecef, \n",
    "                           britdf[\"wiki_lon\"].to_numpy(), \n",
    "                           britdf[\"wiki_lat\"].to_numpy(), \n",
    "                           np.zeros(len(britdf[\"wiki_lat\"])), \n",
    "                           radians=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "britdf[\"x\"] = x\n",
    "britdf[\"y\"] = y\n",
    "britdf[\"z\"] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdtree = spatial.cKDTree(gb1900df[[\"x\", \"y\", \"z\"]].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikix = britdf.iloc[0]['x']\n",
    "wikiy = britdf.iloc[0]['y']\n",
    "wikiz = britdf.iloc[0]['z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neighbors = 5000\n",
    "distance_upper_bound = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dists, all_indxs = kdtree.query(britdf[['x', 'y', 'z']].to_numpy(), \n",
    "                                    k=num_neighbors, \n",
    "                                    distance_upper_bound=distance_upper_bound)\n",
    "print(all_dists)\n",
    "print(all_indxs)\n",
    "\"\"\"\n",
    "for ind in all_indxs:\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(all_indxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "britdf.iloc[160000:160001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dists[160000, 0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qindx = 160000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "dAbbrevs = dict()\n",
    "with open('./abbreviations.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',', quotechar='\"')\n",
    "    next(csv_reader)\n",
    "    for line in csv_reader:\n",
    "        abbrev = line[0]\n",
    "        fulltext = line[1]\n",
    "        or_abbrev = \"\"\n",
    "        if not \",\" in fulltext:\n",
    "            if \" or \" in abbrev:\n",
    "                or_abbrev = abbrev.split(\" or \")\n",
    "            if or_abbrev:\n",
    "                for ab in or_abbrev:\n",
    "                    dAbbrevs[ab] = fulltext\n",
    "            else:\n",
    "                dAbbrevs[abbrev] = fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAbbrevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(corpus):\n",
    "    #corpus = [re.sub(r'[\\.,\\(\\)\\'&:\\[\\]]', '', element,\n",
    "    #                 flags=re.IGNORECASE) for element in corpus]\n",
    "    #corpus = [re.sub(r'\\bthe\\b', '', element, flags=re.IGNORECASE)\n",
    "    #          for element in corpus]\n",
    "    #corpus = [re.sub(r'\\band\\b', '', element, flags=re.IGNORECASE)\n",
    "    #          for element in corpus]\n",
    "    #corpus = [re.sub(r'[\\|-]', ' ', element, flags=re.IGNORECASE)\n",
    "    #          for element in corpus]\n",
    "    corpus = [re.sub(r'[\\.,\\(\\)\\'\\\":\\[\\]-]', '', element,\n",
    "                     flags=re.IGNORECASE) for element in corpus]\n",
    "    corpus = [re.sub(r'\\bthe\\b', '', element, flags=re.IGNORECASE)\n",
    "              for element in corpus]\n",
    "    corpus = [re.sub(r'\\bof\\b', '', element, flags=re.IGNORECASE)\n",
    "              for element in corpus]\n",
    "    corpus = [re.sub(r'\\betc\\b', '', element, flags=re.IGNORECASE)\n",
    "              for element in corpus]\n",
    "    corpus = [re.sub(r'\\&', 'and', element, flags=re.IGNORECASE)\n",
    "              for element in corpus]\n",
    "    corpus = [re.sub(r'\\s+', ' ', element, flags=re.IGNORECASE)\n",
    "              for element in corpus]\n",
    "    corpus = [re.sub(r'\\bst$', 'street', element, flags=re.IGNORECASE)\n",
    "              for element in corpus]\n",
    "    corpus = [re.sub(r'\\bst\\b', 'saint', element, flags=re.IGNORECASE)\n",
    "              for element in corpus]\n",
    "    corpus = [element.strip() for element in corpus]\n",
    "    corpus = [element.lower() for element in corpus]\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup([\"st this is a st mary st\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1900df[\"text2match\"] = cleanup(gb1900df['final_text'])\n",
    "britdf[\"text2match\"] = cleanup(britdf[\"altname\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1900df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "britdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import recordlinkage\n",
    "importlib.reload(recordlinkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In the following list, first specify the method then the weight:\n",
    "# Algorithms: \n",
    "# 'jaro','jarowinkler', 'levenshtein', 'damerau_levenshtein', \n",
    "# 'qgram', 'cosine', 'smith_waterman'\n",
    "# 'longest_common_substring', \n",
    "list_methods_weights = ['jarowinkler', 0,\n",
    "                        'levenshtein', 0,\n",
    "                        'damerau_levenshtein', 0,\n",
    "                        'qgram', 1,\n",
    "                        'cosine', 0,\n",
    "                        # For numeric comparisons (see the figure above), the format is:\n",
    "                        # date_METHOD, e.g.:\n",
    "                        # date_linear\n",
    "                        # date_gauss\n",
    "                        'distance_gauss', 1\n",
    "                       ]\n",
    "list_methods = []\n",
    "list_weights = []\n",
    "for i, ilm in enumerate(list_methods_weights):\n",
    "    if i % 2 == 0:\n",
    "        list_methods.append(ilm)\n",
    "    else:\n",
    "        list_weights.append(ilm)\n",
    "print(\"List of methods: \", list_methods)\n",
    "print(\"List of weights: \", list_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one_brit_index, one_brit in britdf.iterrows():\n",
    "    #print(one_brit_index)\n",
    "    if one_brit_index > 1000:\n",
    "        break\n",
    "    indexer = recordlinkage.Index()\n",
    "    indexer.full()\n",
    "    \n",
    "    one_brit['distance'] = 0.1\n",
    "    #one_brit['distance'] = one_brit['distance'].astype(float)\n",
    "    one_brit_df = pd.DataFrame(one_brit).T\n",
    "    one_brit_df[\"distance\"] = one_brit_df[\"distance\"].astype(float)\n",
    "    \n",
    "    gb1900df_neighbours = gb1900df.iloc[all_indxs[one_brit_index, \n",
    "                                                  all_dists[one_brit_index] <= distance_upper_bound]]\n",
    "    gb1900df_neighbours['distance'] = list(all_dists[one_brit_index, all_dists[one_brit_index] <= distance_upper_bound])\n",
    "    gb1900df_neighbours['distance'] = gb1900df_neighbours['distance'].astype(float)\n",
    "    \n",
    "    candidate_links = indexer.index(one_brit_df, gb1900df_neighbours)\n",
    "\n",
    "    compare_cl = recordlinkage.Compare()\n",
    "    for imethod in list_methods:\n",
    "        if not 'distance' in imethod:\n",
    "            compare_cl.string('text2match', 'text2match', method=imethod, label=imethod)\n",
    "        #else:\n",
    "        #    compare_cl.geo('wiki_lat', 'wiki_lon', \"latitude\", \"longitude\",\n",
    "        #                       method=imethod.split(\"_\")[1], \n",
    "        #                       offset=500.0, scale=1000.0, missing_value=0.5, label=imethod)\n",
    "        else:\n",
    "            compare_cl.numeric('distance', 'distance', method=imethod.split(\"_\")[1], \n",
    "                               offset=1000.0, scale=5000.0, missing_value=0.5, label=imethod)\n",
    "\n",
    "\n",
    "    # The comparison vectors\n",
    "    rl_features = compare_cl.compute(candidate_links, one_brit_df, gb1900df_neighbours)\n",
    "    \n",
    "    for imatch_grp, match_grp in rl_features.groupby(level=0):\n",
    "        match_grp = match_grp.reset_index()\n",
    "    \n",
    "        match_grp['overall_score'] = \\\n",
    "            match_grp.apply(lambda row: \n",
    "                            np.sum([list_weights[i]*row[list_methods[i]] for i in range(len(list_methods))])/np.sum(list_weights),\n",
    "                            axis=1)\n",
    "        max_id = match_grp['overall_score'].idxmax()\n",
    "        max_score = match_grp['overall_score'].max()\n",
    "        if max_score > -1000:\n",
    "            print(\"\\n=============\")\n",
    "            print(\"Overall score                 : {}\".format(round(match_grp.iloc[max_id].overall_score, 3)))\n",
    "            print(np.round(np.array([match_grp.loc[max_id, i] for i in list_methods]), 3))\n",
    "            print(\"Wikipedia (altname) title     : {}\".format(one_brit[\"altname\"]))\n",
    "            print(\"GB1900 (final_text)           : {}\".format(gb1900df.iloc[int(match_grp.iloc[max_id]['level_1'])].final_text))\n",
    "                  \n",
    "                  \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_brit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1900df_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "britdf.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1900df.iloc[151288]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_altname = []\n",
    "l_wiki_title = []\n",
    "l_gb1900text = []\n",
    "l_gb1900lat = []\n",
    "l_gb1900lon = []\n",
    "l_wikilat = []\n",
    "l_wikilon = []\n",
    "for index, row in britdf.iloc[180100:180200].iterrows():\n",
    "    for altnwiki in row['altname'].split(\",\"):\n",
    "        altnwiki = altnwiki.strip()\n",
    "        matches = gb1900df[(gb1900df['final_text'].str.contains(row['altname'], case=False)) & (gb1900df['latitude'] >= row['wiki_lat'] - 0.0) & (gb1900df['latitude'] <= row['wiki_lat'] + 0.1) & (gb1900df['longitude'] >= row['wiki_lon'] - 0.1) & (gb1900df['longitude'] <= row['wiki_lon'] + 0.1)]\n",
    "        for imatch, irow in matches.iterrows():\n",
    "            l_altname.append(altnwiki)\n",
    "            l_wiki_title.append(row['wiki_title'])\n",
    "            l_gb1900text.append(irow['final_text'])\n",
    "            l_gb1900lat.append(irow['latitude'])\n",
    "            l_gb1900lon.append(irow['longitude'])\n",
    "            l_wikilat.append(row['wiki_lat'])\n",
    "            l_wikilon.append(row['wiki_lon'])\n",
    "        if matches.empty:\n",
    "            l_altname.append(altnwiki)\n",
    "            l_wiki_title.append(row['wiki_title'])\n",
    "            l_gb1900text.append('')\n",
    "            l_gb1900lat.append('')\n",
    "            l_gb1900lon.append('')\n",
    "            l_wikilat.append(row['wiki_lat'])\n",
    "            l_wikilon.append(row['wiki_lon'])\n",
    "\n",
    "matchdf = pd.DataFrame(\n",
    "        {'altname' : l_altname,\n",
    "         'wiki_title': l_wiki_title,\n",
    "         'gb1900text': l_gb1900text,\n",
    "         'gb1900lat': l_gb1900lat,\n",
    "         'gb1900lon': l_gb1900lon,\n",
    "         'wiki_lat': l_wikilat,\n",
    "         'wiki_lon': l_wikilon\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchdf.to_pickle(\"match09.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "matchdf = pd.read_pickle(\"match09.pkl\")\n",
    "matchdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close DB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostreSQL:\n",
    "if(gazDB):\n",
    "    cursorGaz.close()\n",
    "    gazDB.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
